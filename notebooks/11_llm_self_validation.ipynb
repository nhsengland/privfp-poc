{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Evaluating the LLM's ability to validate itself\n",
    "This notebook replicates an interesting result from a [recent Microsoft paper](https://arxiv.org/pdf/2303.12712.pdf) which I believe is a reference task used to test the performance of GPT-4 over its predecessors.\n",
    "\n",
    "Figure 1.8 in the paper showss a language model taking structured patient information, turning that into natural prose, and checking its work by verifying each of the claims made in the prose against the original facts. This result was generated using a development version of GPT-4, not the public version we have access to now.\n",
    "\n",
    "We'll first attempt to replicate this result using earlier models, before recreating using the most modern LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import openai\n",
    "\n",
    "from privacy_fingerprint.common.config import (\n",
    "    load_experiment_config_from_file,\n",
    "    load_global_config,\n",
    "    load_global_config_from_file,\n",
    ")\n",
    "from privacy_fingerprint.generate.language_model import LMGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_experiment_config_from_file(\"../experiment_config.yaml\")\n",
    "load_global_config_from_file(\"../global_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading a pre-generated Synthea dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/path/to/synthea_dataset.json\") as fp:\n",
    "    all_records = json.load(fp)\n",
    "print(f\"Read {len(all_records)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this experimentation we use a \"random\" patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating self-assessment using text-da-vinci\n",
    "Start by using the model that has proved very successful in generating convincing looking patient records from structured data - text-da-vinci, one of the 3rd generation of OpenAI models.\n",
    "\n",
    "We will keep our existing generation prompt, and use the Microsoft prompt for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_config = load_global_config()\n",
    "openai.api_key = global_config.openai.api_key\n",
    "\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "example_patient_facts = all_records[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = LMGenerator()\n",
    "generated_records = [\n",
    "    record for record in generator.generate_text([example_patient_facts])\n",
    "]\n",
    "print(generated_records[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_confirmation_prompt(prose, json_format):\n",
    "    \"\"\"Combine the prose and structured data together with a prompt verifying the provided information.\"\"\"\n",
    "    return f\"\"\"Patient's facts:\n",
    "{prose}\n",
    "\n",
    "{json.dumps(json_format, indent=2)}\n",
    "\n",
    "Please read the above medical report and verify that each claim is exactly contained in the patient's facts. Report any information which is not included in, or is missing from, the patient's facts list.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_patient_prose = generated_records[0]\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=assemble_confirmation_prompt(\n",
    "        example_patient_prose, example_patient_facts\n",
    "    ),\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL,\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(example_patient_facts, indent=2))\n",
    "print(example_patient_prose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must manually assess the model performance, as the result is returned as free text. Looking at the above information I agree with the assessment from text-da-vinci that indeed NHS number, address, DoB, visit type, doctor are indeed missing. However the model incorrectly notes gender and visit reason as missing. The latter is quite difficult, as it is easily confused with the condition being \"Chronic sinusitis\". \n",
    "\n",
    "How does this model deal with inserted infromation? I will modify the gender field, and change the visit reason in the prose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_patient_prose = \"\"\"\n",
    "\n",
    "Mr. Cole Monahan is a 62-year-old married female of Mixed - White and Black Caribbean ethnicity who presented to Spire Cosmetic Surgery Clare Park Hospital on June 17th, 1978 with a chief complaint of chest pain. Upon evaluation, it was determined that Mr. Monahan has a diagnosis of chronic sinusitis.\"\"\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=assemble_confirmation_prompt(\n",
    "        modified_patient_prose, example_patient_facts\n",
    "    ),\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL,\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is still incorrect about gender, and does not note the missing visit reason.\n",
    "\n",
    "We can attempt a different prompt to attempt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_confirmation_prompt_instruct(prose, json_format):\n",
    "    \"\"\"Combine the prose and structured data together with a prompt verifying the provided information.\"\"\"\n",
    "    return f\"\"\"Patient's facts:\n",
    "{json.dumps(json_format, indent=2)}\n",
    "\n",
    "Patient record:\n",
    "{prose}\n",
    "\n",
    "Please go through the patient facts one by one, and for each confirm whether it is present in the patient record, returning a JSON object\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "    prompt=assemble_confirmation_prompt_instruct(\n",
    "        modified_patient_prose, example_patient_facts\n",
    "    ),\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL,\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is impressive, as the model has returned a valid JSON object. With templating we may achieve a more accurate result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_templated_prompt_instruct(prose, json_format):\n",
    "    \"\"\"Combine the prose and structured data together with a prompt verifying the provided information.\"\"\"\n",
    "    return f\"\"\"Patient's facts:\n",
    "{json.dumps(json_format, indent=2)}\n",
    "\n",
    "Patient record:\n",
    "{prose}\n",
    "\n",
    "Please go through the patient facts one by one, and for each confirm whether it matches the patient record, is missing from the patient record, or a modification of the patient record, returning a JSON object\n",
    "using the template below:\n",
    "{{\n",
    "  \"name\": ,\n",
    "  \"NHS number\": ,\n",
    "  \"address\": ,\n",
    "  \"date of birth\": ,\n",
    "  \"marital status\": ,\n",
    "  \"ethnicity\": ,\n",
    "  \"gender\": ,\n",
    "  \"visit type\": ,\n",
    "  \"visit date\": ,\n",
    "  \"provider\": {{\n",
    "    \"doctor\": ,\n",
    "    \"facility\": \n",
    "  }},\n",
    "  \"visit reason\": ,\n",
    "  \"conditions\": []\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "    prompt=assemble_templated_prompt_instruct(\n",
    "        modified_patient_prose, example_patient_facts\n",
    "    ),\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL,\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstrates that we can generate very valid templated data, however the output is incorrect. The NHS number, address are missing. However so are DoB, visit type, and doctor. Gender has indeed been modified, but so has the visit reason. We need greater accuracy if we are to trust in such an evaluation process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a chat interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templating_prompt = \"\"\"\n",
    "Please go through the patient facts one by one, and for each confirm whether it matches the patient record, is missing from the patient record, or a modification of the patient record, returning a JSON object\n",
    "using the template below:\n",
    "{{\n",
    "  \"name\": ,\n",
    "  \"NHS number\": ,\n",
    "  \"address\": ,\n",
    "  \"date of birth\": ,\n",
    "  \"marital status\": ,\n",
    "  \"ethnicity\": ,\n",
    "  \"gender\": ,\n",
    "  \"visit type\": ,\n",
    "  \"visit date\": ,\n",
    "  \"provider\": {{\n",
    "    \"doctor\": ,\n",
    "    \"facility\": \n",
    "  }},\n",
    "  \"visit reason\": ,\n",
    "  \"conditions\": []\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def add_to_messages(messages, user_type, message):\n",
    "    messages.append({\"role\": user_type, \"content\": message})\n",
    "\n",
    "\n",
    "def generate_chat_prompt(json_data, prose, validation_prompt):\n",
    "    messages = []\n",
    "    add_to_messages(\n",
    "        messages,\n",
    "        \"system\",\n",
    "        \"You are MedGPT, a helpful assistant carefully creating creating medical notes from structured data, and validating the results.\",\n",
    "    )\n",
    "    add_to_messages(\n",
    "        messages,\n",
    "        \"user\",\n",
    "        f\"Describe this patient as if you were a medical doctor.\\n\\nPatient Facts:\\n{json.dumps(json_data, indent=2)}\\n\",\n",
    "    )\n",
    "    add_to_messages(messages, \"assistant\", f\"Patient Record:\\n{prose}\")\n",
    "    add_to_messages(messages, \"user\", validation_prompt)\n",
    "    return messages\n",
    "\n",
    "\n",
    "for message in generate_chat_prompt(\n",
    "    example_patient_facts, example_patient_prose, templating_prompt\n",
    "):\n",
    "    print(message[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    messages=generate_chat_prompt(\n",
    "        example_patient_facts, example_patient_prose, templating_prompt\n",
    "    ),\n",
    "    model=CHAT_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chat interface does very poorly at this task, and I'm not sure why, especially given the performance of the completion model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this perform in GPT-4, to follow the example provided in the Microsoft paper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = (\n",
    "    \"org-XXXX\"  # Replace with org that has GPT-4 access as necessary\n",
    ")\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    messages=generate_chat_prompt(\n",
    "        example_patient_facts, example_patient_prose, templating_prompt\n",
    "    ),\n",
    "    model=\"gpt-4\",\n",
    ")\n",
    "print(response.choices[0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is correct about the missing NHS number, date of birth, visit type, and address. Let's now test the process on the prose that I have manually modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    messages=generate_chat_prompt(\n",
    "        example_patient_facts, modified_patient_prose, templating_prompt\n",
    "    ),\n",
    "    model=\"gpt-4\",\n",
    ")\n",
    "print(response.choices[0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the model is correct about the gender being a modification, however it now believes the date of birth has been modified - technically it has given it has been replaced with an age, but this is inconsistent. \n",
    "\n",
    "Additionally we now have match, rather than matches, demonstrating how narrow a template must be. Finally we attempt using the original prompt as demonstrated in the Microsoft paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    messages=assemble_templated_prompt_instruct(\n",
    "        modified_patient_prose, example_patient_facts\n",
    "    ),\n",
    "    model=\"gpt-4\",\n",
    ")\n",
    "print(response.choices[0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are impressive, but will be difficult to parse at a large scale. We are limited to demo only usage of the GPT-4 model outside of projects it has been approved for, so must conclude our experiments here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
